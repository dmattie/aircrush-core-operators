#!/usr/bin/env python


from aircrushcore.controller.configuration import AircrushConfig
from aircrushcore.dag import Workload
from aircrushcore.cms.models import *
from os.path import exists,dirname
import os
import importlib
import getpass
from operators import invoke_operator
import datetime
import ast
import subprocess
import argparse

crush_config='crush.ini'
aircrush=AircrushConfig(crush_config)

crush_host=Host(
    endpoint=aircrush.config['REST']['endpoint'],
    username=aircrush.config['REST']['username'],
    password=aircrush.config['REST']['password']
    )

def ready():
    #Sense readiness to do something
    return True

def getOperatorClassDefinition(task_uuid:str):
    
    #This function uses the task uuid to load the associated operator module 
    #and return a class defintion

    task = TaskCollection(cms_host=crush_host).get_one(task_uuid) #fetch task definition
    operator=task.field_operator #identify the operator to execute
    module=f"{task.field_operator}_operator" #build filename for dynamic load
    operator_module=importlib.import_module(f"operators.{module}")  #dynamically import identified operator
    op_class=getattr(operator_module,operator) #get class defintion    
    return op_class

def getMyComputeNodeUUID():

    cluster=aircrush.config['COMPUTE']['cluster']
    account=aircrush.config['COMPUTE']['account']
    working_dir=aircrush.config['COMPUTE']['working_directory'] #os.environ.get("SCRATCH")
    username=getpass.getuser()

    metadata={
        "title":f"{cluster}/{username}",
        "field_host":cluster,
        "field_username":username,
        "field_password":"",
        "field_working_directory":working_dir,        
        "cms_host":crush_host        
    }

    cn_col = ComputeNodeCollection(cms_host=crush_host)
    matching_cn = cn_col.get(filter=f"&filter[field_username][value]={username}&filter[field_host][value]={cluster}")

    for match in matching_cn:          
        metadata['uuid']=match
        break

    n = ComputeNode(metadata=metadata)
    nuid=n.upsert()
    
    return nuid


def pullContainer(uri:str):
    try:        
        container_dir=aircrush.config['COMPUTE']['singularity_container_location']
    except:
        container_dir=aircrush.config['COMPUTE']['working_directory']
    sif = f"{container_dir}/{uri[uri.rfind('/')+1:len(uri)].replace(':','_')}.sif"
    if os.path.isfile(sif):
        print(f"Container exists - will not overwrite ({sif})")
        return sif

    cmdArray=["singularity","pull","--dir",container_dir,uri]
    print(cmdArray)
    ret = subprocess.call(cmdArray)
    
    if not os.path.isfile(sif):
        raise Exception(f"Failed to pull container specified. {sif} not found")
        
    return sif

def pull_source_data(project,subject,session):
    wd=aircrush.config['COMPUTE']['working_directory']
    datacommons=aircrush.config['COMPUTE']['commons_path']
    #Test if we are on an HCP node, use sbatch to perform rsync if so

    root=f"/projects/{project.field_path_to_exam_data}/dataset/source"
    

    #If none of the if statements below match, the entire source will be cloned
    print(f"start root:{datacommons}/{root}")
    if os.path.isdir(f"{datacommons}/{root}/{subject.title}"):
        root=f"{root}/{subject.title}"
        if os.path.isdir(f"{datacommons}/{root}/{session.title}"):
            root=f"{datacommons}/{root}/{session.title}"

    if os.path.isdir(f"{datacommons}/{root}/sub-{subject.title}"):
        root=f"{root}/sub-{subject.title}"
        if os.path.isdir(f"{datacommons}/{root}/ses-{session.title}"):
            root=f"{root}/ses-{session.title}"
            print(f"new root:  {root}")
        
    source_session_dir=f"{datacommons}/{root}"
    target_session_dir=f"{wd}/{root}"

    # if os.path.isdir(target_session_dir):
    #     #It's already there, ignore quietly
    #     print(f"{target_session_dir} Already exists")
    #     return
    # else:
    print(f"Cloning ({source_session_dir}) to local working directory ({target_session_dir})")
    os.makedirs(target_session_dir,exist_ok=True)         

    # ret = subprocess.getstatusoutput("which sbatch")
    # if ret[0]==0:
    #     print("sbatch exists, starting asynchronous copy")
    # else:
    #     print("SBATCH doesn't exist, performing synchronous copy")
        
    if not os.path.isdir(source_session_dir):
        raise Exception(f"Subject/session not found on data commons ({source_session_dir})")
    rsync_cmd=f"rsync -r {source_session_dir} {target_session_dir}"
    print(rsync_cmd)
    
    ret = subprocess.getstatusoutput(rsync_cmd)
    if ret[0]!=0:
        raise Exception(f"Failed to copy session directory: {ret[1]}")
        
def pull_data(stage,project,subject,session):
    if stage=="source":
        pull_source_data(project,subject,session)
    else:
        wd=aircrush.config['COMPUTE']['working_directory']
        datacommons=aircrush.config['COMPUTE']['commons_path']
        #Test if we are on an HCP node, use sbatch to perform rsync if so


        root=f"projects/{project.field_path_to_exam_data}/dataset/{stage}/sub-{subject.title}/ses-{session.title}/"
        source_session_dir=f"{datacommons}/{root}"
        target_session_dir=f"{wd}/{root}"
        # if os.path.isdir(target_session_dir):
        #     #It's already there, ignore quietly
        #     print(f"{target_session_dir} Already exists")
        #     return
        # else:
        print(f"Cloning ({source_session_dir}) to local working directory ({target_session_dir})")
        os.makedirs(target_session_dir,exist_ok=True)         

        # ret = subprocess.getstatusoutput("which sbatch")
        # if ret[0]==0:
        #     print("sbatch exists, starting asynchronous copy")
        # else:
        #     print("SBATCH doesn't exist, performing synchronous copy")
            
        if not os.path.isdir(source_session_dir):
            raise Exception(f"Subject/session not found on data commons ({source_session_dir})")
        rsync_cmd=f"rsync -r {source_session_dir} {target_session_dir}"
        print(rsync_cmd)
        
        ret = subprocess.getstatusoutput(rsync_cmd)
        if ret[0]!=0:
            raise Exception(f"Failed to copy session directory: {ret[1]}")
        
def push_data(stage,project,subject,session):
    if stage=="source":
        print("ERROR: Source data is read-only.  It cannot be pushed back to the data commons")
        return
    else:
        wd=aircrush.config['COMPUTE']['working_directory']
        datacommons=aircrush.config['COMPUTE']['commons_path']
        #Test if we are on an HCP node, use sbatch to perform rsync if so


        root="projects/{project.field_path_to_exam_data}/dataset/{stage}/sub-{subject.title}/ses-{session.title}/"
        source_session_dir=f"{wd}/{root}"
        target_session_dir=f"{datacommons}/{root}"

        print(f"Cloning ({target_session_dir}) to data commons ({source_session_dir})")
        os.makedirs(target_session_dir,exist_ok=True)         

        if not os.path.isdir(source_session_dir):
            raise Exception(f"Subject/session not found on working directory ({source_session_dir})")
        rsync_cmd=f"rsync -r {source_session_dir} {target_session_dir}"
        print(rsync_cmd)
        
        ret = subprocess.getstatusoutput(rsync_cmd)
        if ret[0]!=0:
            raise Exception(f"Failed to copy session directory: {ret[1]}")
        
def parameter_expansion(cmdArray,parms_to_add,**kwargs):
    project=None
    subject=None
    session=None
    workingdir=""
    datacommons=""
    pipeline=""
    print("Parameter expansion")
    if 'project' in kwargs:
        project=kwargs['project']
    if 'subject' in kwargs:
        subject=kwargs['subject']
    if 'session' in kwargs:
        session=kwargs['session']
    if 'workingdir' in kwargs:
        workingdir=kwargs['workingdir']
    if 'datacommons' in kwargs:
        datacommons=kwargs['datacommons']
    if 'pipeline' in kwargs:
        pipeline=kwargs['pipeline']
    if 'datasetdir' in kwargs:
        datasetdir=kwargs['pipeline']
    

    for k in parms_to_add:   

        parm= parms_to_add[k]
        parm = parm.replace("#workingdir",workingdir)
        parm = parm.replace("#datacommons",datacommons)
        parm = parm.replace("#pipeline",pipeline)
        parm = parm.replace("#subject",subject.title)
        parm = parm.replace("#session",session.title)
        parm = parm.replace('#project',project.field_path_to_exam_data)
        para = parm.replace('#datasetdir',f"{workingdir}/projects/{project}/dataset/")
   
        cmdArray.append(f"--{k}")
        cmdArray.append(parm) 
   
    return cmdArray

def doSomething():
    
    nuid = getMyComputeNodeUUID()
    
    
    w=Workload(aircrush) #The list of things to do
    todo = w.get_next_task(node_uuid=nuid) #Do whatever the big brain tells us to do next
    if(todo):  #Todo is a TaskInstance        
        print("-----------------------")  
        task_col =  TaskCollection(cms_host=crush_host)
        task = task_col.get_one(uuid=todo.field_task)    
        #execute(operator=task.field_operator,params=task.field_parameters)
            #Invocation INFO
        messages=[]
        now = datetime.datetime.now()
    

        try:
            updateStatus(todo,"running","","")
            messages.append(f"Invoking operator: {task.field_operator} =====================")        
            messages.append(f"Current date and time: {str(now)}")        
            messages.append(f"Task instance to run: {todo.uuid}")       
            messages.append(f"Container:{task.field_singularity_container}")

            session_col = SessionCollection(cms_host=crush_host)
            session = session_col.get_one(todo.field_associated_participant_ses)
            subject=None
            project=None

            if not session == None:
                subject = session.subject()
                if not subject == None:
                    project = subject.project()

            


            container = pullContainer(task.field_singularity_container)
            workingdir=aircrush.config['COMPUTE']['working_directory']   
            datacommons=aircrush.config['COMPUTE']['commons_path']     
            cmdArray=["singularity","run","--app",task.field_operator,container]  
            parms = ast.literal_eval(task.field_parameters)

            # pullSession(project,subject,session)

            # # pulldata
            # print(f"Pulling any necessary data for operation")            
            # for k in parms:
            #     if parms[k]=="#source":
            #         pull_data("source",project,subject,session)
            #     if parms[k]=="#rawdata":
            #         pull_data("rawdata",project,subject,session)
            #     if parms[k]=="#derivatives":
            #         pull_data("derivatives",project,subject,session)

            cmdArray = parameter_expansion(cmdArray,parms,
                datacommons=datacommons,
                workingdir=workingdir,
                project=project,
                subject=subject,
                session=session)
             
            print(cmdArray)  
            ret = subprocess.run(cmdArray,stdout=subprocess.PIPE,stderr=subprocess.PIPE, universal_newlines=True)
#            ret = subprocess.call(cmdArray)
            
            messages.append(f"Operation executed:{cmdArray}") 
            print(f"Exit code:{ret.returncode}")
            messages.append("*************************************************")
            messages.append("*************************************************")
            messages.append(ret.stdout)
            messages.append("*************************************************")
            messages.append("*************************************************")
            
            messages.append(f"Current date and time: {str(now)}")                 
            messages.append("End of Operation=======================\n")
            if ret.returncode==0:
                updateStatus(todo,"completed",'<br/>\n'.join(messages),ret.stderr)
            else:
                updateStatus(todo,"failed",'<br/>\n'.join(messages),ret.stderr)
        except Exception as e:
            print(e)
            if hasattr(e, 'message'):
                new_errors=e.message
            else:
                new_errors=str(e)
            messages.append(f"Current date and time: {str(now)}")  
            updateStatus(todo,"failed",'<br/>\n'.join(messages),new_errors)
        


       # op = invoke_operator(id=todo.uuid,cms_host=crush_host)


        #op_class = getOperatorClassDefinition(todo.field_task) #use the task UUID to determine which operator to use    
        #op = op_class(id=todo.uuid,cms_host=crush_host) #instantiate it based on task instance    
       # op.execute(aircrush)  #Let's do this!
    else:
        print("Nothing to do.")

    # if(todo):  #Todo is a TaskInstance        
    #     print("-----------------------")        
    #     op_class = getOperatorClassDefinition(todo.field_task) #use the task UUID to determine which operator to use    
    #     op = op_class(id=todo.uuid,cms_host=crush_host) #instantiate it based on task instance    
    #     op.execute(aircrush)  #Let's do this!
    # else:
    #     print("Nothing to do.")



#def execute(operator:str,params:str):#**kwargs):

def updateStatus(task_instance,status:str,detail:str="",new_errors:str=""):
    # Valid statuses
    ###########################
    # notstarted,running,failed,completed
    task_instance.field_status=status        
    task_instance.body = detail
    task_instance.field_errorlog = new_errors

    uuid=task_instance.upsert()

def main():
    if ready():
        doSomething()

if __name__ == '__main__':
    main()


   